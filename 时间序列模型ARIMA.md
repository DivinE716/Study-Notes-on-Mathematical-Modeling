# 时间序列模型ARIMA

## 1. 模型介绍

AR模型，即自回归模型，其优势是对于具有**较长历史趋势**的数据，AR模型可以捕获这些趋势，并据此进行预测。但是AR模型不能很好地处理某些类型的时间序列数据，例如那些有临时、突发的变化或者噪声较大的数据。AR模型相信“历史决定未来”，因此很大程度上忽略了现实情况的复杂性、也忽略了真正影响标签的因子带来的不可预料的影响。

相反地，MA模型，即移动平均模型，可以更好地处理那些**有临时、突发的变化或者噪声较大**的时间序列数据。但是对于具有较长历史趋势的数据，MA模型可能无法像AR模型那样捕捉到这些趋势。MA模型相信“时间序列是相对稳定的，时间序列的波动是由偶然因素影响决定的”，但现实中的时间序列很难一直维持“稳定”这一假设。

基于以上两个模型的优缺点，我们引入了ARIMA模型，这是一种结合了AR模型和MA模型优点的模型，可以处理更复杂的时间序列问题。

要学习时间序列模型，首先我们要理解并区分**时期序列**和**时点序列**。
1. **时期序列**中的观测值反映现象在一段时期内发展过程的总量，不同时期的观测值**可以相加**，相加结果表明现象在更长一段时间内的活动总量，如中国过去10年的GDP序列；
2. **时点序列**中的观测值反映现象在某一瞬间上所达到的水平，不同时期的观测值**不能相加**，相加结果没有实际意义，如某地每隔一小时测得的温度数据。

ARIMA的建模思路大致如下：

![ARIMA模型大纲](/imgs/2025-10-21/xpN6co0D9f09Au2H.png)

即：
1. 对序列绘图，进行平稳性检验，观察序列是否平稳；对于非平稳时间序列要先进行d 阶差分,转化为平稳时间序列；
2. 过第一步处理，已经得到平稳时间序列。要对平稳时间序列分别求得其自相关系数（ACF）和偏自相关系数（PACF），通过对自相关图和偏自相关图的分析或通过AIC/BIC搜索，得到最佳的阶数p、q；
3. 以上得到的d、q、P，得到 ARIMA 模型。然后开始对得到的模型进行模型检验。

## 2. 模型公式推导

先回顾一下AR和MA模型的数学表达式: 

$$AR: Y_t = c + \varphi_1 Y_{t-1} + \varphi_2 Y_{t-2} + \dots + \varphi_p Y_{t-p} + \xi_t$$

$$MA: Y_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \dots + \theta_q \epsilon_{t-q}$$ 

如果我们暂时不考虑差分（即假设d=0），那么ARIMA模型可以被看作是AR模型和MA模型的直接结合，形式上看，ARIMA模型的公式可以表示为：

$$Y_t = c + \varphi_1 Y_{t-1} + \varphi_2 Y_{t-2} + \dots + \varphi_p Y_{t-p} + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \dots + \theta_q \epsilon_{t-q} + \epsilon_t$$

在这个公式中：
- $Y_t$ 是我们正在考虑的时间序列数据。
- $\varphi_1$到 $\varphi_p$是AR模型的参数，这些参数用来描述当前值与过去p个时间点值之间的关系。
- $\theta_1$到 $\theta_q$是MA模型的参数，这些参数用来描述当前值与过去q个时间点的误差之间的关系。
- $\epsilon_t$是在t时间点的误差项。
- $c$是一个常数项。

这个公式基本上是将AR模型和MA模型的公式组合在一起: 

1、AR部分（即 $\varphi_1 Y_{t-1} + \varphi_2 Y_{t-2} + \dots + \varphi_p Y_{t-p}$）表示当前值 $Y_t$ 与它过去的值有关，这个部分的形式与AR模型的公式一致。

2、MA部分（即 $\theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \dots + \theta_q \epsilon_{t-q}$）表示当前值 $Y_t$ 与它过去的误差项有关，这个部分的形式与MA模型的公式一致。

不难发现，MA模型中代表长期趋势的均值 $\mu$ 并不存在于ARIMA模型的公式当中，因为ARIMA模型中“预测长期趋势”这部分功能由AR模型来执行，因此AR模型替代了原本的 $\mu$ 。值得注意的是，在ARIMA模型中， $c$ 可以为0。

另外，这个公式的基础是假设我们正在处理的时间序列是平稳的，这样我们可以直接应用AR和MA模型。如果时间序列是非平稳的，那么我们就需要考虑ARIMA模型中的“I”部分，也就是进行差分处理。

上述模型被称之为ARIMA(p,d,q)模型，其中p和q的含义与原始MA、AR模型中完全一致，且p和q可以被设置为不同的数值，而d是ARIMA模型需要的差分的阶数。

## 3. ARIMA中的三个参数
### 3.1 d（差分）

差分的过程往往包含两个参数，阶数和滞后，其中阶数即为我们要重点考察的参数 $d$。

差分的本质就是对“相邻”项作差，这里的“相邻”指的是相隔步长为**滞后项**的一对数据。此外，阶数指示的是差分运算的次数，即n阶差分就是在原始数据基础上进行n次一阶差分。在现实中，我们使用的高阶差分一般阶数不会太高。在ARIMA模型中，超参数 $d$最常见的取值是0、1、2这些很小的数字。

总结起来就是：

1. **差分（Differencing）**：这是一种预处理技术，用于使非平稳时间序列变得平稳。在时间序列中进行一阶差分，就是将每个观察值与其前一步的观察值进行比较，然后取这两个观察值之间的差异。例如，如果我们有一个时间序列 $x_1, x_2, x_3, \dots, x_n$，那么一阶差分序列将是 $x_2 - x_1, x_3 - x_2, \dots, x_n - x_{n-1}$。 
2. **滞后差分（Lagged Differencing）**：这个术语和"差分"非常相似。当我们说"滞后"时，我们是在说比较一个观察值和其"前一步"或"几步前"的观察值。因此，"滞后一阶差分"实际上就是常规的一阶差分，因为我们比较的是每个观察值与其前一步的观察值。如果我们进行的是"滞后k阶差分"，那么我们比较的是每个观察值与其k步前的观察值。
3. **n阶差分（n-th Order Differencing）**：n阶差分是差分的一种更一般的形式。一阶差分是比较每个观察值与其前一步的观察值，二阶差分是对一阶差分序列进行再一次的差分（也就是比较一阶差分序列中的每个值与其前一步的值）。更一般地，n阶差分就是连续进行n次一阶差分。
4. **多步差分（Multi-step Differencing）**：这个术语可能根据上下文有不同的含义。它可能指的是n阶差分（即进行多次连续的一阶差分）。也可能指的是滞后差分，比如比较每个观察值与其几步前的观察值。

综上所述，差分运算可以消除数据中激烈的波动，因此可以消除时间序列中的季节性、周期性、节假日等影响。一般我们使用滞后为7的差分消除星期的影响，而使用滞后为12的差分来消除月份的影响（一般这种情况下每个样本所对应的时间单位是月），我们也常常使用滞后4来尝试消除季度所带来的影响。在统计学中，差分运算本质是一种信息提取方式，其最擅长提取的关键信息就是数据中的**周期性**，和其他信息提取方式一样，它会舍弃部分信息、提炼出剩下的信息供模型使用。也因此，差分最重要的意义之一就是能够**让带有周期性的数据变得平稳**。

在实际使用中，我们经常将多步差分和高阶差分混用，最典型的就是在ARIMA模型建模之前：一般我们会先使用多步差分令数据满足ARIMA模型的基础建模条件，再在ARIMA模型中使用低阶的差分帮助模型更好地建模。例如，先对数据进行12步差分、再在模型中进行1阶差分，这样可以令数据变得平稳的同时、又提取出数据中的周期性，极大地提升模型对数据的拟合精度。

### 3.2 p和q
在 ARIMA(p, d, q) 模型中: 

**p 代表 "自回归部分 (Autoregressive)"**：这部分描述了模型中使用的观测值的滞后值（即前面 p 个期的值）。自回归模型的出发点是认为观测值是它前面的 p 个值的线性组合。具体的数学形式如下: 

$$AR: Y_t = c + \varphi_1 Y_{t-1} + \varphi_2 Y_{t-2} + \dots + \varphi_p Y_{t-p} + \xi_t$$

其中， $\varphi_1, \varphi_2, \dots, \varphi_p$ 是模型参数，分别衡量滞后 1 期、滞后 2 期…… 滞后 p 期的因变量 $Y_{t-1}, Y_{t-2}, \dots, Y_{t-p}$对当前值  $Y_t$的影响程度。c 是常数， $\xi_t$ 是白噪声。这个方程的阶数 p 决定了模型回溯观测值的数量。 

**q 代表 "移动平均部分 (Moving Average)"**：这部分描述了模型中使用的错误项的滞后值（即前面 q 个期的值）。移动平均模型是将当前值和过去的白噪声之间建立关系。具体的数学形式如下: 

$$MA: Y_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \dots + \theta_q \epsilon_{t-q}$$

其中， $\theta_1, \theta_2, \dots, \theta_q$ 是模型参数，c 是常数， $\epsilon_t$ 是当前时期的白噪声， $\epsilon_{t-1}, \epsilon_{t-2}, \dots, \epsilon_{t-q}$ 是过去的白噪声。这个方程的阶数 q 决定了模型回溯白噪声的数量。 

因此，ARIMA 模型将自回归模型（AR）和移动平均模型（MA）结合在一起，同时加入了差分（I）这个操作。而 p, d, q 这三个参数，分别代表了模型中的自回归部分、差分阶数、以及移动平均部分。

## 4. ACF与PACF
在时间序列分析中，我们通常需要将非平稳时间序列转化为平稳时间序列，因为许多时间序列模型（如AR、MA和ARIMA模型）都假设输入的数据是平稳的。这种转化可以通过差分或其他预处理方法来实现。

ACF (Auto-Correlation Function)和PACF (Partial Auto-Correlation Function)是时间序列分析中的两个重要工具，它们可以用来检验一个时间序列是否是平稳的，以及帮助确定ARIMA模型的参数。

### 4.1 自相关函数ACF (Auto-Correlation Function)
在实际应用中，ACF通常被定义为**当前时间点上的观测值**与**历史时间点观测值**之间的相关性。这种相关性可以用多种方法来衡量，其中最常用的是皮尔逊相关系数。

这是一个相对宽泛的定义，而在时间序列分析中，ACF有着更为严格的定义。对于任意的滞后（lag）k，我们都计算出在时间t和时间t+k的数据点之间的**协方差**，然后除以该时间序列的**方差**。这样得到的结果反映了时间序列自身的相关性。

数学上，自相关函数（ACF）的定义如下：

如果我们有一个时间序列 $X_t$，那么对于任意的滞后（lag）k，自相关函数 $\rho(k)$可以表示为：

$$\rho(k)=\frac{Cov(X_t,X_{t+k})}{Var(X_t)}$$

其中， $Cov(X_t,X_{t+k})$是时间点 t 和时间点 t+k 的观测值的协方差， $Var(X_t)$是时间序列 $X_t$的方差。

ACF 的取值范围是 -1 到 1。当 ACF 接近 1 时，表示两个时间点的观测值高度正相关；当 ACF 接近 -1 时，表示两个时间点的观测值高度负相关；当 ACF 接近 0 时，表示两个时间点的观测值之间的相关性较弱。

通过计算不同滞后值下的 ACF，我们可以得到一个关于滞后的函数，这就是自相关函数。我们通常使用自相关图（ACF 图）来直观地表示这个函数。

### 4.2 偏自相关函数（Partial Auto-correlation Function， PACF）
PACF（偏自相关系数）是用来衡量时间序列里，两个相隔一段时间的数值之间 “纯粹的” 直接关系，排除了中间那些时间点数值的干扰。

要通俗理解，我们可以拆成两个关键概念来看。

#### 1. 先搞懂 “直接相关性”

“直接相关性” 就是不看其他因素，只看两个东西本身的关联。举个生活例子：假设 A 和 C 是朋友，但 A 和 B 是好朋友，B 和 C 也是好朋友。

-   如果只看 A 和 C 走得近，可能是因为两人本身合得来（直接相关），也可能是因为都想跟 B 玩（间接相关）。
-   “直接相关性” 要算的，就是 A 和 C 抛开 B 的影响后，本身关系有多好。

考虑一个时间序列 $X_t$：分析第 10 个数值 $x_{10}$和第 8 个数值 $x_{8}$的直接相关性时，会先把第 9 个数值 $x_{9}$的影响去掉，只算它俩本身的关联度。

#### 2. PACF：时间序列里的 “纯关系检测器”

在时间序列分析里，每个数值都和过去的数值有关联（比如今天的气温和昨天、前天的都有关）。PACF 的作用就是 “提纯” 这种关联。

它的逻辑可以简单拆成两步：

1.  **先 “过滤” 干扰**：比如要算今天（t）和大前天（t-3）的直接相关性，会先把昨天（t-1）、前天（t-2）这两个中间时间点的影响全部排除。
2.  **再算 “纯关联”**：过滤完之后，剩下的就是今天和大前天之间 “不掺水分” 的直接相关性，这个数值就是 PACF 值。

---

数学上，偏自相关函数（PACF）的定义如下：

如果我们有一个时间序列 ${X_t}$，那么对于任意的滞后（lag）k，偏自相关函数 $\varphi(k)$ 可以表示为：

$$$





>参考：[时间序列模型(四)：ARIMA模型 - 知乎](https://zhuanlan.zhihu.com/p/634120397)
<!--stackedit_data:
eyJoaXN0b3J5IjpbMjMzNzk4MjAxLDk2NTI4NDA4NSw0MzI0ND
I4NDEsMTEyNDM1Njg4OCwxODAyNDY4MzczLC0zMTI5Nzc1OTUs
NTY5ODkxNjg2LDQyMjg0ODUwM119
-->